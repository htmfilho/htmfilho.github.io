---
layout: post
title: "ICMI-MLMI 2009 at MIT"
date: 2009-11-04 15:52:00 +0200
categories: uncategorized conference research user interface
---

I’m visiting Boston, USA, right now to attend the Eleventh International Conference on Multimodal Interfaces and the Sixth Workshop on Machine Learning for Multimodal Interaction – <a href="http://icmi2009.acm.org/">ICMI-MLMI 2009</a>. This is the most important conference on the field of Multimodal Interaction, which is one of my PhD’s case studies.

<div style="clear: both; text-align: center;"><a href="http://69.89.31.239/~hildeber/wp-content/uploads/2009/11/DSC02514.jpg" style="clear: right; float: right; margin-bottom: 1em; margin-left: 1em;">![DSC02514-225x300.jpg](/images/posts/DSC02514-225x300.jpg)</a></div>
Yesterday I made my presentation entitled “A Fusion Framework for Multimodal Interactive<br/>Applications”, which is one of the applications of my PhD research. I never spent time in this blog to explain what is multimodal interaction and neither multimodal fusion, but let me do it shortly now: <b>multimodal interaction</b> is the possibility to interact with systems using more than one modality (vision, auditory, touching, etc.) and each modality has a different or complementary influence on the interaction. For example: an text processing software that supports keyboard and also speech recognition as ways to input text. <b>Multimodal fusion</b> is the possibility to use information from two or more modalities, in order to improve a less precise modality or realise more complex meanings by combining data from those modalities. For example: to get better results on speech recognition, a vision system could be used to perform lips reading, improving the probability that a certain word was said in case of uncertainty.

Well, my presentation was good and their questions were tough but they were satisfied with my answers and some of them even sent emails in private, asking for the presentation. I would say that this was the toughest audience that I’ve ever had. A conference at MIT normally attracts many big names in the field of multimodal interaction. The environment is really appropriate (and magic) to discuss about research and innovation. This year the conference had a big emphasis on robotics, with exciting demos and impressive results on interaction and learning with humans. We also had the chance to make a tour through the lab, visiting different projects and watching live demonstrations (some pictures below).

<div style="clear: both; text-align: center;"><a href="http://69.89.31.239/~hildeber/wp-content/uploads/2009/11/DSC02500.jpg" style="margin-left: 1em; margin-right: 1em;">![DSC02500-300x225.jpg](/images/posts/DSC02500-300x225.jpg)</a></div>
<div style="text-align: center;"><i>One of the dozens of rooms in the MIT Media Lab</i></div>


<div style="clear: both; text-align: center;"><a href="http://69.89.31.239/~hildeber/wp-content/uploads/2009/11/DSC02526.jpg" style="margin-left: 1em; margin-right: 1em;">![DSC02526-225x300.jpg](/images/posts/DSC02526-225x300.jpg)</a></div>
<div style="text-align: center;"><i>Social and emotional interaction with a child robot</i></div>
The experience of visiting the <a href="http://www.media.mit.edu/">MIT Media Lab</a> was unique. This is a space where people are totally free to create even if ideas sound totally ridiculous. At the end, they find a way to exploit those ideas. It shows that, sometimes, real problems can also block people’s creativity and great ideas also come from nowhere.
