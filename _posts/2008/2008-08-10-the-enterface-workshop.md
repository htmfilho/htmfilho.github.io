---
layout: post
title: "The eNTERFACE Workshop"
date: 2008-08-10 19:50:00 +0200
categories: conference europe research user interface
---

Imagine an event where people from different parts of the world meet in a great touristic place to work during one month in a project of their choice. This is an [eNTERFACE workshop](http://www.enterface.net), a special event conceived by [Thierry Dutoit](http://tcts.fpms.ac.be/~dutoit/) in the context of an European project called [Similar Network of Excellence](http://www.similar.cc/).

I’m talking about eNTERFACE because I’m part of it now. Just to contextualize you, I’m working in an Electrical Engineering Lab at Université catholique de Louvain and my advisor, Benoit Macq, is the head of the Similar Network. Thierry Dutoit and him have been colleagues for a long time and Thierry is also part of the network. Thierry created the eNTERFACE model 3 years ago and he promoted the first eNTERFACE workshop in the [Faculté Polytechnique de Mons](http://www.fpms.ac.be/) in the summer of 2005. It was the beginning of a successful event on the field of [Human Computer Interaction](http://en.wikipedia.org/wiki/Human-computer_interaction), which was hosted in Dubrovnik Croatia in 2006, Istanbul Turkey in 2007 and now we are in its [4th edition](http://enterface08.limsi.fr) in Orsay-Paris, France.

![PIE_0304-300x199.jpg](/images/posts/PIE_0304-300x199.jpg) eNTERFACE Workshop

A collegue from my lab, called Olga Vybornova, submitted a project to the workshop and I was invited to be the architect. The project was accepted at the end of 2007, which means that we were one of the projects that people can apply to work on. So, the second step was to analyze many CVs of people who were interested in our project. We chose 3 of them to work in our team during the workshop.

The title of our project is “Multimodal High Level Data Integration”, which means we will fuse 2 different modalities in order to help people on their every day life. The two modalities are speech and human behavior, detected by a speech recognition tool and an image processing tool respectively. The fusion mechanism will combine speech and movements to predict what people are planning to do in a certain scenario and the computer should help them to complete their tasks as fast as possible. For instance: If you say something like “I would like to talk with Nick” and at the same time, or just a moment later, you walk on the direction of the telephone, the computer should tell you Nick’s phone number. Then, you don’t have to look for his number in your contact list. It is indeed a very challenging project, but we have all the elements to make it work until the end of August, including a plan B, C, D …..

The workshop started in August 4th and we finished the first week with good results. We also had a speech given by Lotfi A. Zadeh, the creator of Fuzzy Logic :O. It was great and he deserves a complete post just about him and his talk.

![PIE_0288-300x199.jpg](/images/posts/PIE_0288-300x199.jpg) Lotfi Zadeh at eNTERFACE